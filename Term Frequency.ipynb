{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "def compute_tf(words):\n",
    "    term_counts = Counter(words)\n",
    "    total_terms = len(words)\n",
    "    tf = {word: count / total_terms for word, count in term_counts.items()}\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Inverse Document Frequency (IDF)\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    all_words = set(word for doc in documents for word in doc)\n",
    "    idf = {}\n",
    "    \n",
    "    for word in all_words:\n",
    "        containing_docs = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = np.log((1 + N) / (1 + containing_docs)) + 1\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "def compute_tfidf(tf, idf):\n",
    "    tfidf = {word: tf_val * idf[word] for word, tf_val in tf.items()}\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize_tfidf(tfidf):\n",
    "    norm = math.sqrt(sum(value ** 2 for value in tfidf.values()))\n",
    "    if norm == 0:\n",
    "        return tfidf\n",
    "    normalized_tfidf = {term: value / norm for term, value in tfidf.items()}\n",
    "    return normalized_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top words\n",
    "def display_top_words(tfidf, top_n=10):\n",
    "    sorted_tfidf = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_tfidf[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode documents\n",
    "def encode_documents(documents):\n",
    "    idf = compute_idf(documents)\n",
    "    tfidf_vectors = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        tf = compute_tf(doc)\n",
    "        tfidf = compute_tfidf(tf, idf)\n",
    "        normalized_tfidfs = normalize_tfidf(tfidf)\n",
    "        tfidf_vectors.append(normalized_tfidfs)\n",
    "    \n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process all PDFs\n",
    "pdf_files = [\"Cooking_Recipe.pdf\", \"Martial_Arts.pdf\", \"Tennis.pdf\", \"NLP.pdf\"]\n",
    "doc_texts = [extract_text_from_pdf(pdf) for pdf in pdf_files]\n",
    "doc_words = [preprocess_text(text) for text in doc_texts]\n",
    "\n",
    "tfidf_vectors = encode_documents(doc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in Document 1: [('pasta', 0.33004599984777094), ('sauce', 0.33004599984777094), ('simple', 0.22003066656518064), ('fresh', 0.22003066656518064), ('basil', 0.22003066656518064), ('cooking', 0.11001533328259032), ('recipe', 0.11001533328259032), ('tomato', 0.11001533328259032), ('classic', 0.11001533328259032), ('easy', 0.11001533328259032)]\n",
      "Top words in Document 2: [('martial', 0.39314935196840256), ('arts', 0.39314935196840256), ('sports', 0.39314935196840256), ('selfdefense', 0.19657467598420128), ('combatbased', 0.09828733799210064), ('combine', 0.09828733799210064), ('strength', 0.09828733799210064), ('discipline', 0.09828733799210064), ('practiced', 0.09828733799210064), ('centuries', 0.09828733799210064)]\n",
      "Top words in Document 3: [('tennis', 0.3359616161831774), ('open', 0.3359616161831774), ('sport', 0.2239744107887849), ('ball', 0.2239744107887849), ('court', 0.2239744107887849), ('making', 0.1429599651341907), ('globally', 0.11198720539439246), ('involves', 0.11198720539439246), ('two', 0.11198720539439246), ('four', 0.11198720539439246)]\n",
      "Top words in Document 4: [('nlp', 0.46619362075982046), ('language', 0.3729548966078563), ('used', 0.18647744830392815), ('natural', 0.18647744830392815), ('text', 0.18647744830392815), ('speech', 0.18647744830392815), ('large', 0.18647744830392815), ('data', 0.18647744830392815), ('processing', 0.09323872415196408), ('field', 0.09323872415196408)]\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "for i, tfidf in enumerate(tfidf_vectors):\n",
    "    print(f\"Top words in Document {i+1}:\", display_top_words(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words across all documents: [('nlp', 0.24663181183349467), ('martial', 0.19730544946679573), ('arts', 0.19730544946679573), ('sports', 0.19730544946679573), ('making', 0.19730544946679573), ('language', 0.19730544946679573), ('pasta', 0.1479790871000968), ('sauce', 0.1479790871000968), ('tennis', 0.1479790871000968), ('open', 0.1479790871000968), ('simple', 0.09865272473339787), ('requires', 0.09865272473339787), ('fresh', 0.09865272473339787), ('basil', 0.09865272473339787), ('physical', 0.09865272473339787), ('selfdefense', 0.09865272473339787), ('techniques', 0.09865272473339787), ('popular', 0.09865272473339787), ('coordination', 0.09865272473339787), ('mental', 0.09865272473339787)]\n"
     ]
    }
   ],
   "source": [
    "# Combine all words from the four documents\n",
    "all_words = [word for doc in doc_words for word in doc]\n",
    "\n",
    "# Compute TF-IDF for combined words\n",
    "combined_tf = compute_tf(all_words)\n",
    "combined_idf = compute_idf([all_words])  # Treat as a single document\n",
    "combined_tfidf = compute_tfidf(combined_tf, combined_idf)\n",
    "normalized_combined_tfidf = normalize_tfidf(combined_tfidf)\n",
    "\n",
    "# Print top 20 words for all documents combined\n",
    "print(\"Top 20 words across all documents:\", display_top_words(normalized_combined_tfidf, top_n=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
